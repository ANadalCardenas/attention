{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLQPJbJLlBq7"
      },
      "source": [
        "# Automatic Differentiation with PyTorch\n",
        "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) ([UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) 2019)\n",
        "\n",
        "Updated by [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro) ([UPC TelecomBCN](https://telecombcn-dl.github.io/dlai-2019/) 2019) and [Gerard I. GÃ¡llego](https://www.linkedin.com/in/gerard-gallego/)\n",
        "\n",
        "## Course material\n",
        "* [Slides](https://www.slideshare.net/xavigiro/backpropagation-for-neural-networks) by [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro)\n",
        "* [Video](https://www.youtube.com/watch?v=uub_hqDlqjc) by [Elisa Sayrol](https://imatge.upc.edu/web/people/elisa-sayrol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6S_xlm3AI05"
      },
      "source": [
        "This session will be about how to perform backpropagation in PyTorch. To build neural networks with PyTorch we must first understand how this framework simplifies our life. Central to all neural networks in PyTorch is the `autograd` package [1]. This package provides **automatic differentiation for all operations on Tensors**. *HOW COOL IS THAT?*\n",
        "\n",
        "This means you can put layers and layers of operations over your PyTorch tensors, and the `autograd` package already computes the derivatives of those operations in the backprop process. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different. *Wow wow, wait... define-by-run? every single iteration can be different? What is all this jargon??* Let's see the following concepts during this tutorial:\n",
        "\n",
        "* The `grad`s in our `tensor`s.\n",
        "* The dynamic computational graph concept (DCG).\n",
        "* The `.backward()` life saver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFV0p9ORSbPa"
      },
      "source": [
        "# Imports that will be needed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2IIIbEwUV0C"
      },
      "source": [
        "### Long Story Short\n",
        "\n",
        "The `torch.Tensor` class has an attribute `.requires_grad`. If you set it to `True`, it **starts tracking all operations on it**. When you finish your computations you can call `.backward()` and have **all the gradients computed automatically**. The gradient for this tensor will be **accumulated** into the `.grad` attribute.\n",
        "\n",
        "*IMPORTANT: Accumulated means it sums up the new gradients to the already existing ones (if any)!*\n",
        "\n",
        "Any operation performed on a `Tensor` that conforms a `Function` (`torch.autograd.Function`) creates a new node of an acyclic graph. This means that each `Tensor` where it comes from (its source `Tensor` and the `Function` that created itself). The `Tensor` attribute `grad_fn` references the `Function` that created it. THAT SIMPLE.\n",
        "\n",
        "Example multiplication of two tensors and the resulting interconnections [2]:\n",
        "\n",
        "![img](https://miro.medium.com/max/336/1*jGo_2J9UQeynwG_3olUD4w.png)\n",
        "\n",
        "Well this is the so called *dynamic computational graph (DCG)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbGvYeNdljpR"
      },
      "source": [
        "#### Creating a tensor, operating on it, and computing derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmQl0Bq4SbPf"
      },
      "source": [
        "import torch\n",
        "\n",
        "def describe_tensor(tensor, name=''):\n",
        "  # Helper function to explore the attributes of a tensor object\n",
        "  print('-' * 30)\n",
        "  print('Name: ', name)\n",
        "  print('-' * 30)\n",
        "  print('data : ', tensor.data)\n",
        "  print('requires_grad : ', tensor.requires_grad)\n",
        "  print('grad: ', tensor.grad)\n",
        "  print('grad_fn: ', tensor.grad_fn)\n",
        "  print('is_leaf: ', tensor.is_leaf)\n",
        "  print('=' * 30)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSK_M8_-SbPg",
        "outputId": "5c2d40a4-d6c0-4069-cc43-28670c51bb56"
      },
      "source": [
        "# create a tensor x\n",
        "x = torch.tensor(1.0)\n",
        "# create a tensor y\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "describe_tensor(x, name='x')\n",
        "describe_tensor(y, name='y')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  x\n",
            "------------------------------\n",
            "data :  tensor(1.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  y\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJNJTDNJSbPh",
        "outputId": "94b36015-4398-4ec9-8bff-5d5768ab7cac"
      },
      "source": [
        "# Create z as the multiplicative outcome of x * y\n",
        "z = x * y\n",
        "describe_tensor(z, name='z')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  z\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQlbc0IHH7a"
      },
      "source": [
        "We have created a DCG out of a simple product of two scalar tensors. But there is no node requiring gradients! Let's call the life saver `backward()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "JG5WJHBcSbPi",
        "outputId": "bbc54bec-4b9b-4e38-bcc3-25437b7c2ede"
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-148511205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYMY1ke3ejl3"
      },
      "source": [
        "No hesitation, is was meant to crash. There is no `Tensor` requiring to track the graph because none required the gradients to be computed with `requires_grad=True`. Now we can make it require the gradients by simply using the inplace function `.require_gradients_(True)` or by specifying the flag as `True` at `Tensor` creation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOPAaUfTSbPj",
        "outputId": "cd22a0a1-0f4c-4055-9e46-20e89bd3d888"
      },
      "source": [
        "# make x track gradients\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "z = x * y\n",
        "describe_tensor(z)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  \n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  <MulBackward0 object at 0x7e5b92b6ead0>\n",
            "is_leaf:  False\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1325539916.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('grad: ', tensor.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60YjCAse8yQ"
      },
      "source": [
        "#### Note the change in the Tensor description!\n",
        "\n",
        "Now there are two important differences from this `z` to the previous one. First, this one DOES require gradient tracking. But secondly, it contains a `grad_fn` reference to a `MulBackward` operation! Which is basically telling us that multiplication will go through a derivative process in the backward step when we call the `backward()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVjQtYXXSbPk",
        "outputId": "650e62b3-6469-4141-b9f1-4e930967a834"
      },
      "source": [
        "# call .backward() now on z\n",
        "z.backward()\n",
        "\n",
        "# Now describe each tensor x, y and z\n",
        "describe_tensor(x, 'x')\n",
        "describe_tensor(y, 'y')\n",
        "describe_tensor(z, 'z')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  x\n",
            "------------------------------\n",
            "data :  tensor(1.)\n",
            "requires_grad :  True\n",
            "grad:  tensor(2.)\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  y\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  z\n",
            "------------------------------\n",
            "data :  tensor(2.)\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  <MulBackward0 object at 0x7e5b92b6ead0>\n",
            "is_leaf:  False\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1325539916.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('grad: ', tensor.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDt1X6nKh1up"
      },
      "source": [
        "Note that the cell above is showing the text description of the graph [2]:\n",
        "\n",
        "![img](https://miro.medium.com/max/471/1*viCEZbSODfA8ZA4ECPwHxQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJ3x95wfhlA"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Where does the result of `x.grad` come from and why does it have this value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK_S-fY3rHD6"
      },
      "source": [
        "#### If you try `z.backward()` it will crash, notice the message"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "wq4ZuohtSbPl",
        "outputId": "504dd1fc-ac78-442f-a3a3-a5614a40bd4f"
      },
      "source": [
        "z.backward()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-148511205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAyOlfD6sMDM"
      },
      "source": [
        "#### When the backward computation is done...\n",
        "\n",
        "The DCG is removed, and so we cannot perform backprop anymore. Unless you specify you want to retain the graph to do as many backwards as desired (for whatever reason and for an advanced usage of PyTorch).\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Make use of the `retain_graph` flag in the `backward` call to backpropagate twice a tensor of ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6HezNPgSbPl"
      },
      "source": [
        "# Let's try to backward twice\n",
        "\n",
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0)\n",
        "z = x * y\n",
        "\n",
        "# TODO: Backward twice\n",
        "z.backward(retain_graph=True)\n",
        "\n",
        "z.backward()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-ZoEsbvA6I"
      },
      "source": [
        "## Building a Neural Network and Training it\n",
        "\n",
        "We will now build a neural network to exemplify the simplicity of using PyTorch for deep learning. And then we will see how backpropagation is applied on it. The network will have one hidden layer and one output layer. We use the `nn` package in PyTorch to get to the neural components, also called `Module`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbMgCnq5SbPm"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # must call the superclass init first\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\n",
        "    self.fc1 = nn.Linear(3, 20)\n",
        "    # First hidden activation\n",
        "    self.act1 = nn.Tanh()\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\n",
        "    self.fc2 = nn.Linear(20, 3)\n",
        "    # No activation as we make it a linear output\n",
        "\n",
        "  def forward(self, x):\n",
        "    # activation of first layer is Tanh(FC1(x))\n",
        "    h1 = self.act1(self.fc1(x))\n",
        "    # output activation\n",
        "    y = self.fc2(h1)\n",
        "    return y"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgk-kKENSbPm",
        "outputId": "83e2b0f2-57a1-4227-bd84-66f4371e2d29"
      },
      "source": [
        "# We instantiate our network now, and can even print its structure\n",
        "net = MyNet()\n",
        "print(net)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNet(\n",
            "  (fc1): Linear(in_features=3, out_features=20, bias=True)\n",
            "  (act1): Tanh()\n",
            "  (fc2): Linear(in_features=20, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvjlbynRSbPm",
        "outputId": "3c31bc04-5595-48a4-a32e-1da19f346086"
      },
      "source": [
        "# We can explore the weight tensor of a layer very simply\n",
        "describe_tensor(net.fc1.weight, 'FC1 weight')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  FC1 weight\n",
            "------------------------------\n",
            "data :  tensor([[ 0.3647, -0.2693,  0.2063],\n",
            "        [ 0.1853, -0.1636, -0.4604],\n",
            "        [ 0.3011, -0.0923,  0.1869],\n",
            "        [ 0.0794, -0.4369,  0.0658],\n",
            "        [ 0.3441, -0.3006,  0.3927],\n",
            "        [ 0.1208,  0.1611,  0.4026],\n",
            "        [ 0.2598,  0.1727,  0.0783],\n",
            "        [ 0.4306, -0.0153,  0.2192],\n",
            "        [-0.2186,  0.1484,  0.3953],\n",
            "        [-0.3519,  0.4718,  0.0505],\n",
            "        [ 0.2166,  0.3677, -0.0009],\n",
            "        [-0.4077, -0.5542,  0.4359],\n",
            "        [-0.1419,  0.4448, -0.0747],\n",
            "        [-0.2522, -0.1530, -0.4291],\n",
            "        [ 0.1073, -0.5250,  0.0890],\n",
            "        [ 0.2627,  0.4016, -0.3360],\n",
            "        [-0.2430, -0.4013, -0.3941],\n",
            "        [ 0.4336,  0.5710,  0.3903],\n",
            "        [-0.2517, -0.2293, -0.4178],\n",
            "        [-0.1899, -0.0305, -0.2868]])\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dToaGYVRxqcW"
      },
      "source": [
        "Observe that by default we have that the fully connected layer `fc1` DOES require the gradient computation. It is evident that they simplify our lives, because that is the last node to be reached in the backpropagation stage and we do not even have to take care of explicitly saying so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqD9-YzRSbPn",
        "outputId": "1411057b-547e-46f0-d6a4-3063257a39be"
      },
      "source": [
        "# We can access all the parameters of our network with the .parameters() function, that returns an iterable\n",
        "# over all tunnable params we created.\n",
        "params = list(net.parameters())\n",
        "for p in params:\n",
        "  print(p.shape)\n",
        "print('You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 3])\n",
            "torch.Size([20])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3])\n",
            "You should see two matrices (weights, OUTxIN) and two vectors (biases, OUT). Each pair of weight (W) and bias (b) comes from a fully connected layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBmJP_z0sTu"
      },
      "source": [
        "We will use mean squared error (MSE) as the loss function to be able to compute the error between our network outputs and some labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr9tXvSHSbPn"
      },
      "source": [
        "loss_fn = F.mse_loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-WeLqGhy80p"
      },
      "source": [
        "Now remember that training a neural network usually takes the following steps:\n",
        "\n",
        "1. Make a forward pass with some input `x` to activate each layer until output `y_`\n",
        "2. Compute the error towards a label `y` with a loss function (like MSE for example)\n",
        "3. Backpropagate the gradients through the network (`.backward()` call)\n",
        "4. Update every tunnable network parameter with its `.grad` attribute (using some optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIGbGkyi08dm"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "We will instantiate a network like the one shown earlier and train it to map simple uniform noise to zeros. We will track the loss value, which must decrease, and will plot it.\n",
        "\n",
        "1. **Understand why we call `optimizer.zero_grad()`** in the training loop. Read its functionallity in the PyTorch documentation: https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer\n",
        "2. **Fill in the missing pieces to complete the aforementioned training steps** in order to observe a decreasing loss in the depicted plot. The loss should get very close to zero with a clear decreasing trend in few iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "CwUezZK_SbPo",
        "outputId": "6c8f92d7-75b1-4ee3-eab1-950eed85ad2e"
      },
      "source": [
        "def train(network, optimizer, loss_fn, num_iters):\n",
        "  \"\"\" Training function \"\"\"\n",
        "\n",
        "  loss_history = []\n",
        "\n",
        "  for niter in range(1, num_iters + 1):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # TODO: sample 10 (minibatch size) random samples\n",
        "    # of dimension expected by NN\n",
        "    # (https://pytorch.org/docs/stable/torch.html#torch.rand)\n",
        "    x = torch.rand(10, 3)\n",
        "\n",
        "    # 1) Forward the data through the network\n",
        "    y_ = network(x)\n",
        "\n",
        "    # 2) Compute the loss wrt to a zero label\n",
        "    loss = loss_fn(y_, torch.zeros(y_.shape))\n",
        "\n",
        "    # 3) Backprop with respect to the loss function\n",
        "    # TODO\n",
        "    loss.backwards(retain_graph=True)\n",
        "\n",
        "    # Store the loss log to plot\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # 4) Apply the optimizer with a learning step\n",
        "    optimizer.step()\n",
        "\n",
        "    if niter % 50 == 0:\n",
        "      print('Step {:2d} loss: {:.3f}'.format(niter, loss_history[-1]))\n",
        "\n",
        "  plt.plot(loss_history)\n",
        "  plt.xlabel('Niter')\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "net = MyNet()\n",
        "# we will take stochastic gradient descent (SGD) to exemplify the training loop of a neural network\n",
        "# We first need to handle the parameters that the optimizer will tune, and then we must specify the learning rate (lr) of each\n",
        "# update step\n",
        "opt = optim.SGD(net.parameters(), lr=0.01)\n",
        "train(net, opt, loss_fn, 500)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'backwards'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3367256460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3367256460.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, optimizer, loss_fn, num_iters)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 3) Backprop with respect to the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Store the loss log to plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'backwards'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA23afv_wJq9"
      },
      "source": [
        "We can compare the difference in output from a non-trained and the trained network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzTRYmf0SbPp"
      },
      "source": [
        "# Generate 5 random samples of dimensionality 3\n",
        "x_sample = torch.rand(5, 3)\n",
        "\n",
        "# Forward the data through a non trained network\n",
        "non_trained = MyNet()\n",
        "print('Non-trained result: ', torch.mean(non_trained(x_sample)).item())\n",
        "\n",
        "# Forward through the trained network (net)\n",
        "print('Trained result: ', torch.mean(net(x_sample)).item())\n",
        "\n",
        "print('Trained result should be closer to zero than the non-trained one (if training went well).')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY2OxOiH9ZQ2"
      },
      "source": [
        "### When we do NOT want gradients\n",
        "\n",
        "There are some scenarios where we want to avoid building the `backward` graph, as we will not need gradients. For example, during inference/prediction/test. We can avoid the computation of gradients through the neural network forward pass by enclosing it into the `with torch.no_grad()` context (**which speeds up evaluation process by x2 or x3 normally**). As an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMWGmFNdSbPp",
        "outputId": "8f85f47e-f937-469b-bc9e-140f52807fbc"
      },
      "source": [
        "x = torch.zeros(10, 3)\n",
        "with torch.no_grad():\n",
        "  y_ = net(x)\n",
        "  loss = loss_fn(y_, torch.zeros(x.shape))\n",
        "  print('Loss: {:.2f}'.format(loss))\n",
        "  describe_tensor(loss, 'loss')\n",
        "  print('NOTE THAT requires_grad=False NOW IN THE LOSS TENSOR')\n",
        "  # This would crash: y.backward()\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.12\n",
            "------------------------------\n",
            "Name:  loss\n",
            "------------------------------\n",
            "data :  tensor(0.1171)\n",
            "requires_grad :  False\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "NOTE THAT requires_grad=False NOW IN THE LOSS TENSOR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNyoimhfACrG"
      },
      "source": [
        "Finally, we can also cut the graph at any point we want (if we want) with the `.detach()` function of a `Tensor`. For instance, if we only wanted to train the output layer in the previous network (leaving the first layer `fc1` to behave randomly for whatever reason), we can re-define it as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdW__-ZbSbPp",
        "outputId": "c9250f43-5157-499b-d72b-2fc0a8f343e7"
      },
      "source": [
        "class MyNetWithDetach(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # must call the superclass init first\n",
        "    # First fully-connected layer (3 inputs, 20 hidden neurons)\n",
        "    self.fc1 = nn.Linear(3, 20)\n",
        "    # First hidden activation\n",
        "    self.act1 = nn.Tanh()\n",
        "    # Second fully-connected layer (20 hidden neurons, 3 outputs)\n",
        "    self.fc2 = nn.Linear(20, 3)\n",
        "    # No activation as we make it a linear output\n",
        "\n",
        "  def forward(self, x):\n",
        "    # activation of first layer is Tanh(FC1(x))\n",
        "    h1 = self.act1(self.fc1(x))\n",
        "    # DETACH\n",
        "    h1 = h1.detach()\n",
        "    # output activation\n",
        "    y = self.fc2(h1)\n",
        "    return y\n",
        "\n",
        "# Now we can train this network\n",
        "net = MyNetWithDetach()\n",
        "# Now we can observe the difference of gradients in the biases of the 2 layers\n",
        "# between this network and the regular one\n",
        "# in terms of computed gradients\n",
        "\n",
        "def forward_backward(network, net_name=''):\n",
        "  x = torch.zeros(10, 3)\n",
        "  y_ = network(x)\n",
        "  loss = loss_fn(y_, torch.zeros(x.shape))\n",
        "  loss.backward()\n",
        "  describe_tensor(network.fc1.bias, '{}:FC1 bias'.format(net_name))\n",
        "  describe_tensor(network.fc2.bias, '{}:FC2 bias'.format(net_name))\n",
        "\n",
        "# Try with a non-detached network\n",
        "forward_backward(MyNet(), 'Non-Detached Net')\n",
        "# Try with a detached network\n",
        "forward_backward(MyNetWithDetach(), 'Detached Net')\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Name:  Non-Detached Net:FC1 bias\n",
            "------------------------------\n",
            "data :  tensor([-0.2796,  0.3599, -0.0288,  0.2884, -0.0959, -0.4886, -0.2021,  0.1027,\n",
            "        -0.5027,  0.3110,  0.4218, -0.1397,  0.0508,  0.5605,  0.5634,  0.3197,\n",
            "        -0.4838,  0.3740,  0.5687,  0.3472])\n",
            "requires_grad :  True\n",
            "grad:  tensor([ 0.0095,  0.0305,  0.0379, -0.0154, -0.0028, -0.0250, -0.0459,  0.0265,\n",
            "        -0.0132, -0.0336, -0.0303,  0.0221, -0.0052, -0.0079,  0.0188,  0.0152,\n",
            "        -0.0195, -0.0212, -0.0255,  0.0055])\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  Non-Detached Net:FC2 bias\n",
            "------------------------------\n",
            "data :  tensor([0.1055, 0.0307, 0.2170])\n",
            "requires_grad :  True\n",
            "grad:  tensor([0.1278, 0.0693, 0.0668])\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  Detached Net:FC1 bias\n",
            "------------------------------\n",
            "data :  tensor([-0.3726,  0.2299, -0.0359, -0.5113, -0.0671, -0.0462,  0.1850, -0.2867,\n",
            "        -0.2692,  0.3181,  0.2208,  0.5517, -0.5410,  0.5222, -0.2660, -0.1188,\n",
            "        -0.1989,  0.2528,  0.2148, -0.5753])\n",
            "requires_grad :  True\n",
            "grad:  None\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n",
            "------------------------------\n",
            "Name:  Detached Net:FC2 bias\n",
            "------------------------------\n",
            "data :  tensor([-0.0320,  0.0929,  0.1763])\n",
            "requires_grad :  True\n",
            "grad:  tensor([-0.1043,  0.1099,  0.1967])\n",
            "grad_fn:  None\n",
            "is_leaf:  True\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-8_-TyB8Ju"
      },
      "source": [
        "And you may wonder...\n",
        "\n",
        "1. *Why would I cut the gradient flow at a certain point in my neural network? It looks like avoiding the learning process in some components. How may this be beneficial?* **A: Well, a neural network can be trained per blocks, as they are also tunnable feature extractors. Just bear in mind that you can bring a pre-trained neural network piece, attach it to your own additional piece, and tune only your own part of the network by freezing the first one**.\n",
        "\n",
        "2. *What happens to the optimizer, which has the full list of parameters of my network, after I detach the graph?* **A: the optimizer still contains a reference to your parameters. So in the MyNetWithDetach case, it still has a reference to fc1 parameters. Nonetheless, as `.grad` is None, it simply cannot update the parameters.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLjQHWuk06p"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
        "\n",
        "[2] https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n",
        "\n",
        "[3] https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html"
      ]
    }
  ]
}