{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANadalCardenas/attention/blob/main/%5BAIDL_ONL_2025_26%5D_L01_Preprocessing_(todo).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzf7wMo7993"
      },
      "source": [
        "# Sentiment Analysis: Data preprocessing\n",
        "\n",
        "In this notebook we are going to compare different approaches of data preprocessing applied to a real world task, analysing movie reviews given by IMBD users. Each review can be classified in two different classes, positive, if the user likes the movie and negative otherwise. This tutorial is inspired by: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcDdxME79i5"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "Our first step will be to load and prepare all the our data to perform the experiments. In this case we are going to employ IMBD reviews data available at The Training Dataset used is stored in the zipped folder: aclImbdb.tar file. This can also be downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "The dataset consists in 50.000 sentences splitted in two sets, train and test of 25.000 sentences each.\n",
        "\n",
        "For our experiments the corpus will be splitted in the following way:\n",
        "* Train_split: 17500 sentences extracted from the original training data.\n",
        "* Validation split: 7500 sentences extracted from the original training data.\n",
        "* Test split: 25000 sentences that form the original test data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install datasets\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "_p3HEFI9iM6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90ab27ae-60ae-4617-f736-2df6cc087549"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.32.4)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchdata) (3.0.3)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdPNvE6578X3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e5ff3f5-b160-484f-bf68-048ec37220fb"
      },
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]\n",
        "train_split = list(train_data)\n",
        "test_split = list(test_data)\n",
        "train_sents = [t[\"text\"].split() for t in train_split]\n",
        "test_sents = [t[\"text\"].split() for t in test_split]\n",
        "\n",
        "train_targets = [t[\"label\"] for t in train_split]\n",
        "test_targets = [t[\"label\"] for t in test_split]\n",
        "\n",
        "\n",
        "train_sents, valid_sents, train_targets, valid_targets = train_test_split(train_sents,\n",
        "                                                                          train_targets,\n",
        "                                                                          test_size=0.25,\n",
        "                                                                          random_state=SEED)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3NNt_G78Pw"
      },
      "source": [
        "In order to feed our data into a model that is able to predict the sentiment of our movie reviews we should first create a vocabulary of all the words that appear in our training data.\n",
        "\n",
        "A measure of quality of our vocabulary can be its coverage over the data. We can define the vocabulary coverage as the percentage of tokens from our data that are found in our vocabulary.\n",
        "\n",
        "As we will see later, vocabulary size is a paremeter that can be tuned and this coverage can serve as guidande.\n",
        "\n",
        "**Compute the vocabulary as a dictionary wirh words as keys and the number of times the word appears as value.**\n",
        "\n",
        "**Compute the coverage over the data given our vocabulary.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYSHNxa078HT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d0f8ed51-617e-4034-8a68-86055a41e68e"
      },
      "source": [
        "def compute_vocabulary(train_sents):\n",
        "  vocabulary = {'<unk>':99999999}\n",
        "  for sent in train_sents:\n",
        "    for word in sent:\n",
        "      if word in vocabulary:\n",
        "        vocabulary.update({word:vocabulary[word]+1})\n",
        "      else:\n",
        "        vocabulary[word] = 1\n",
        "\n",
        "  return vocabulary\n",
        "\n",
        "def coverage(split,voc):\n",
        "  total_words = 0\n",
        "  words_in_voc = 0\n",
        "  for sent in split:\n",
        "    for word in sent:\n",
        "      total_words += 1\n",
        "      if word in voc:\n",
        "        words_in_voc += 1\n",
        "  if total_words == 0:\n",
        "    return 0.0\n",
        "  return (words_in_voc / total_words) * 100\n",
        "\n",
        "def voc_stats(split,voc):\n",
        "  print('**** VOCABULARY ***')\n",
        "  print('* Unique words', len(voc))\n",
        "  print('* Coverage', coverage(split,voc))\n",
        "\n",
        "voc = compute_vocabulary(train_sents)\n",
        "voc_stats(valid_sents, voc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 234010\n",
            "* Coverage 96.5128798732154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OncZryV877-b"
      },
      "source": [
        "Our first proposed preprocess is word level tokenization. In this cases all word will be splitted and stop-words will be separated. In this case we are going to employ 'nltk' library for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j9vfkqo771Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0b3cd9d2-1d4c-4d40-929e-9cac852502d9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "stop_words = list(stopwords.words('english')) #About 150 stopwords\n",
        "print(stop_words)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    return  nltk.word_tokenize(sentence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlEBwZY977sO"
      },
      "source": [
        "**Compute the tokenization and vocabulary given the function avobe. How the vocabulary size and the coverage have changed?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KpDN5d_77ie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f988bcd1-2a79-41a3-c401-0725ab39bd7c"
      },
      "source": [
        "train_tok_sents = [tokenize_sentence(' '.join(s)) for s in train_sents]\n",
        "valid_tok_sents = [tokenize_sentence(' '.join(s)) for s in valid_sents]\n",
        "test_tok_sents = [tokenize_sentence(' '.join(s)) for s in test_sents]\n",
        "voc = compute_vocabulary(train_tok_sents)\n",
        "voc_stats(valid_tok_sents, voc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 113196\n",
            "* Coverage 98.76523292554803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbvTTK-V77Zu"
      },
      "source": [
        "\n",
        "In addition to the previous steps we are going further preprocess our data.\n",
        "\n",
        "For each review in the corpus we apply a preprocess consisting in:\n",
        "\n",
        "- Each word is replaced by its lemma form. Lemmatization reduces vocabulary size as all different forms of a word are grouped in a single lemma.\n",
        "- Remove casing from words, this helps reduce ambiguity as upper and lower cased appearences of a word are selected as different in the vocabulary.\n",
        "- Remove stop words. In order to reduce sentence length and represent the words that carry the meaning of the sentence, we remove all stop words.\n",
        "\n",
        "**Apply the preprocess and compute its new vocabulary. How did stats change?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7MVF0vK77QK"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    return  [lemmatizer.lemmatize(word.lower()) for word in sentence if not word in stop_words]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tok_sents = [preprocess_sentence(s) for s in train_tok_sents]\n",
        "valid_tok_sents =  [preprocess_sentence(s) for s in valid_tok_sents]\n",
        "test_tok_sents =  [preprocess_sentence(s) for s in test_tok_sents]\n",
        "\n",
        "tok_voc = compute_vocabulary(train_tok_sents)\n",
        "voc_stats(valid_tok_sents,tok_voc)"
      ],
      "metadata": {
        "id": "q8AvkUFUBvR-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1951b501-111d-4e3a-a7e2-de1ef5fed45d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 89198\n",
            "* Coverage 98.50315916593148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmDMCuz8768V"
      },
      "source": [
        "In both cases vocabularies are too big to be handled in this task as a lot of words only appear a few times in the dataset of some ambiguity is still present in the data.\n",
        "\n",
        "**Create a 5000 token vocabulary that only include the most frequent tokens in the dataset. How is the new coverage?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ZLjZFl76zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ad8b499c-3642-4130-ef2c-473a5a4fc7c0"
      },
      "source": [
        "def reduce_vocabulary(voc,size=5000):\n",
        "  sorted_voc = dict(sorted(voc.items(), key=lambda item: item[1], reverse=True))\n",
        "  return dict(itertools.islice(sorted_voc.items(), size))\n",
        "\n",
        "SIZE_5000 = 5000\n",
        "tok_voc = reduce_vocabulary(tok_voc,size=SIZE_5000)\n",
        "voc = reduce_vocabulary(voc,size=SIZE_5000)\n",
        "voc_stats(valid_tok_sents,tok_voc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 5000\n",
            "* Coverage 87.82846712153088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVXR-DW76rW"
      },
      "source": [
        "The previous preprocess helps to reduce the ambiguity produced by the different\n",
        "form of the same word or stop-words included in other tokens, but the original tokens are mostly unchanged. In the following cases we will explore other levels of tokenization, characters and subwords.\n",
        "\n",
        "Let's start with characters, where all words in the dataset are stripped into its individual characters, which has several new characterisitics:\n",
        "* Vocabularies are orders of magnitude smaller that its word counterparts, as all words share the same script that uses a limited set of them.\n",
        "* Each token does not provide a lot of information about the sentence. Individual words include semantic and morpghological information.\n",
        "* Character's use is more ambiguious, words are used generally on the same context consistently during the dataset while characters can belong to a great nunmber of words.\n",
        "\n",
        "**Based on the previous tokenization compute a character vocabulary over the training, and measure its size and coverage of the validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l2ifQu576hq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1b33d6aa-b6f5-4741-fd6d-1f5269737a15"
      },
      "source": [
        "\n",
        "train_char_sents = [ch for sent in train_sents for word in sent for ch in word]\n",
        "valid_char_sents =  [ch for sent in valid_sents for word in sent for ch in word]\n",
        "test_char_sents =  [ch for sent in test_sents for word in sent for ch in word]\n",
        "\n",
        "char_voc =  compute_vocabulary(train_char_sents)\n",
        "voc_stats(valid_char_sents,char_voc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 167\n",
            "* Coverage 99.99928760856154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUgsWe_76V_"
      },
      "source": [
        "Character level may be too extreme for some tasks, but it provides a great coverage of the dataset. In those cases a great alternative e to apply is subword tokenization.\n",
        "\n",
        "In this case words are splitted in pieces which lenght depends on how common they are in the dataset. This way long pieces that are really common will be maintained, for example lemmas of common words in the data, while for not common words or affixes the vocabulary includes smaller pieces until arriving to  individual characters.\n",
        "\n",
        "This tokenizations allows:\n",
        "* A great coverage of the data, as only words including characters not present in the training data will not be recognized.\n",
        "* A parametrized vocabulary size. The number of tokens in the vocabulary is set when the tokenization is computed and can be tuned to improve the models performance.\n",
        "\n",
        "The example shows a call to Byte-Pair-Encoding tokenization using the standard subword-nmt library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVZde6La76Jn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "efa6d1bf-1597-4654-ebd3-958a56395b83"
      },
      "source": [
        "! pip install subword-nmt\n",
        "\n",
        "# Write the data splits into files to compute the Byte-Pair Encoding (BPE)\n",
        "with open('train.txt','w+') as tr:\n",
        "  for s in train_tok_sents:\n",
        "    print(' '.join(s),file=tr)\n",
        "\n",
        "with open('valid.txt','w+') as vl:\n",
        "  for s in valid_tok_sents:\n",
        "    print(' '.join(s),file=vl)\n",
        "\n",
        "with open('test.txt','w+') as ts:\n",
        "  for s in test_tok_sents:\n",
        "    print(' '.join(s),file=ts)\n",
        "\n",
        "\n",
        "# Compute BPE codes and apply to the data splits\n",
        "! subword-nmt learn-bpe -s 5000 < train.txt > bpe5000.codes\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < train.txt > train.bpe.txt\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < valid.txt > valid.bpe.txt\n",
        "! subword-nmt apply-bpe -c bpe5000.codes < test.txt > test.bpe.txt\n",
        "\n",
        "train_bpe_sents = []\n",
        "with open('train.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    train_bpe_sents.append(line.replace('\\n','').split())\n",
        "\n",
        "  valid_bpe_sents = []\n",
        "with open('valid.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    valid_bpe_sents.append(line.replace('\\n','').split())\n",
        "\n",
        "test_bpe_sents = []\n",
        "with open('test.bpe.txt') as tr:\n",
        "  for line in tr.readlines():\n",
        "    test_bpe_sents.append(line.replace('\\n','').split())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.12/dist-packages (from subword-nmt) (5.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from subword-nmt) (4.67.1)\n",
            "100% 5000/5000 [00:11<00:00, 425.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMlq8XJU75_o"
      },
      "source": [
        "**Compute the coverage and vocabulary size of the subword tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt6QCC_Vpp6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d2f6a701-c130-468a-a2b2-51f1d7e46e87"
      },
      "source": [
        "bpe_voc = compute_vocabulary(train_bpe_sents)\n",
        "voc_stats(valid_bpe_sents,bpe_voc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 5200\n",
            "* Coverage 99.99653748834731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained Tokenizers from Language Models:**\n",
        "\n",
        "Pretrained tokenizers are components of large, pretrained language models (like BERT, GPT, or RoBERTa). These tokenizers come with a pre-built vocabulary and a set of rules for splitting text into tokens (e.g., words or subwords). They have been trained on massive amounts of text data and are designed to work seamlessly with their corresponding language models.\n",
        "\n",
        "Using a pretrained tokenizer has two main advantages:\n",
        "\n",
        "\n",
        "*   Consistency: The tokenizerâ€™s vocabulary and rules match those used during the\n",
        "\n",
        "*   model's training, ensuring that your text is processed in the same way.\n",
        "Convenience: Instead of building your own tokenizer and managing a vocabulary manually, you can simply load an existing one using libraries such as Hugging Face Transformers.\n",
        "\n"
      ],
      "metadata": {
        "id": "tdXVye0MCmyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def tokenize_with_pretrained(model_name, sentences):\n",
        "    # Load the tokenizer corresponding to the given model name.\n",
        "    # This retrieves a pretrained tokenizer with its associated vocabulary and tokenization rules.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Check if the sentences are provided as lists of tokens.\n",
        "    # If so, join them into raw text strings (because the tokenizer expects plain text).\n",
        "    if isinstance(sentences[0], list):\n",
        "        sentences = [\" \".join(s) for s in sentences]\n",
        "\n",
        "    # Tokenize the sentences:\n",
        "    # - 'padding=True' ensures all sequences are padded to the same length.\n",
        "    # - 'truncation=True' limits sequences that are too long.\n",
        "    # - 'max_length=512' sets the maximum allowed length of each tokenized sentence.\n",
        "    encoded = tokenizer(sentences,\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        max_length=512)\n",
        "\n",
        "    # Convert the token IDs back into token strings.\n",
        "    # This returns a list where each element corresponds to the tokenized representation of a sentence.\n",
        "    return [tokenizer.convert_ids_to_tokens(ids) for ids in encoded[\"input_ids\"]]\n"
      ],
      "metadata": {
        "id": "xlQfHzn7Cj9l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize the sentences with the BERT tokenizer**\n"
      ],
      "metadata": {
        "id": "Md3y0SBRDCvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize with BERT-base-uncased and XLM-RoBERTa-base\n",
        "bert_tokenized_train = tokenize_with_pretrained(\"bert-base-uncased\", train_sents)\n",
        "# xlmr_tokenized_train = tokenize_with_pretrained(\"xlm-roberta-base\", train_tok_sents)\n",
        "\n",
        "bert_tokenized_valid = tokenize_with_pretrained(\"bert-base-uncased\", valid_sents)\n",
        "# xlmr_tokenized_valid = tokenize_with_pretrained(\"xlm-roberta-base\", valid_tok_sents)\n",
        "\n",
        "bert_tokenized_test = tokenize_with_pretrained(\"bert-base-uncased\", test_sents)\n",
        "# xlmr_tokenized_test = tokenize_with_pretrained(\"xlm-roberta-base\", test_tok_sents)"
      ],
      "metadata": {
        "id": "V1opqnnyCqvU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_voc = compute_vocabulary(bert_tokenized_train)\n",
        "voc_stats(bert_tokenized_valid,bert_voc)"
      ],
      "metadata": {
        "id": "cPXwMUnXDHMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7a39a34b-e30b-454d-e102-a9f9687b4101"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** VOCABULARY ***\n",
            "* Unique words 24552\n",
            "* Coverage 99.9781875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2BHcot75rj"
      },
      "source": [
        "# Classification Task\n",
        "\n",
        "In this final section we are going to measure the performance of a model when using as features the different tokenization levels described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGM2drNf75hE"
      },
      "source": [
        "The first step in order to train a model is decide how to feed our data into the model. For these experiments, we are going to employ bag of words vectors as they do not require any further preprocess.\n",
        "\n",
        "Bag of Words (BOW) consists in representing each of the sentences of the dataset as fixed size vectors of the size of the vocabulary. For example, for our 5000 tokens vocabularies, each sentence will be represented as a 5000 dimension vectors, following these steps:\n",
        "\n",
        "- Initially all vector dimensions are initialized as 0.\n",
        "- For each word in the sentence, 1 is added to the position of such word in the vocabulary.\n",
        "\n",
        "The final vector is a sparce vector, most of its values are 0, that contains the number of times each word appears in the sentence and the sum of all dimensions of the vector is the number of tokens in the sentence.\n",
        "\n",
        "Note, that while this representation provides a fixed size representation of the sentences it lacks information of the order in which words appear.\n",
        "\n",
        "**Create a general model to compute the vocabulary indexes and the BOW representations of our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHhUM1xD75WK"
      },
      "source": [
        "def idx_voc(voc):\n",
        "  items = list(voc.items())\n",
        "  items.sort(key=lambda x: x[1], reverse=True)\n",
        "  return {item[0]:n for n,item in enumerate(items)}\n",
        "\n",
        "def bag_of_words(splits,voc):\n",
        "  voc_indexed = idx_voc(voc)\n",
        "  all_bow_splits = []\n",
        "  for split_data in splits:\n",
        "    current_bow_split = []\n",
        "    for sent in split_data:\n",
        "      bow = np.zeros(len(voc_indexed))\n",
        "      for word in sent:\n",
        "        if word in voc_indexed:\n",
        "          bow[voc_indexed[word]] += 1\n",
        "      current_bow_split.append(bow)\n",
        "    all_bow_splits.append(current_bow_split)\n",
        "  return all_bow_splits[0], all_bow_splits[1], all_bow_splits[2]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvQKOit775Kj"
      },
      "source": [
        "As a classifier we are going to employ Random forest (https://towardsdatascience.com/understanding-random-forest-58381e0602d2), a model that consists in an ensemble of Decision Trees that see different data by means of bagging and boosting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you use a pretrained BERT tokenizer, it splits text into subword units rather than whole words. Applying a simple bag-of-words model on these subwords can lead to an extremely large and fragmented vocabulary, resulting in very high-dimensional and sparse representations. This dense representation, if not handled correctly, can be both computationally and memory inefficient. In contrast, TF-IDF creates a sparse matrix that only stores non-zero entries and also weights tokens by their importance across the corpus. This sparse format is more memory-friendly and better suited for handling the many subword tokens produced by BERT tokenization.\n",
        "\n",
        "**TF-IDF Vectorization:**\n",
        "The TfidfVectorizer is used to convert the text into a numerical representation.\n",
        "When fitting on the training texts with vectorizer.fit_transform(train_texts), the vectorizer learns the vocabulary and calculates the inverse document frequency (IDF) for each token.\n",
        "The test texts are then transformed using vectorizer.transform(test_texts) based on the vocabulary learned from the training data.\n",
        "The parameter max_features=5000 limits the number of features (tokens) to 5000 to control memory usage and potentially reduce noise.\n",
        "\n",
        "\n",
        "*   For character-level processing:\n",
        "Each sentence (which is a list of characters) is joined using \"\".join(sent) to create a continuous string without spaces. This ensures that each character is preserved in sequence.\n",
        "\n",
        "*   For word-level processing:\n",
        "Each sentence (a list of word tokens) is joined with spaces using \" \".join(sent). This creates a typical sentence string where words are separated by spaces, which is what the vectorizer expects.\n",
        "\n"
      ],
      "metadata": {
        "id": "tKx0ol7xDO_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_tfidf(splits, char_level=False):\n",
        "    \"\"\"\n",
        "    Computes TF-IDF features for training and testing datasets.\n",
        "\n",
        "    Parameters:\n",
        "        splits (list): A list of three elements where:\n",
        "                        - splits[0] is the training data,\n",
        "                        - splits[1] is the validation data (unused here),\n",
        "                        - splits[2] is the test data.\n",
        "                        Each element should be a list of sentences, and each sentence is a list of tokens.\n",
        "        char_level (bool): If True, use character-level tokenization; otherwise, use word-level tokenization.\n",
        "\n",
        "    Returns:\n",
        "        train_tfidf, test_tfidf: The TF-IDF transformed training and test data.\n",
        "    \"\"\"\n",
        "    if char_level:\n",
        "        # If char_level is True, we configure the vectorizer to break text into characters.\n",
        "        # analyzer='char' means that individual characters (or character n-grams) are the tokens.\n",
        "        # ngram_range=(1, 1) specifies that we only consider individual characters.\n",
        "        # max_features limits the vocabulary size to 5000 to avoid memory issues.\n",
        "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 1), max_features=5000)\n",
        "        # For character-level analysis, join each sentence's characters without spaces.\n",
        "        # This produces a continuous string where each character is kept in order.\n",
        "        train_texts = [\"\".join(sent) for sent in splits[0]]\n",
        "        test_texts = [\"\".join(sent) for sent in splits[2]]\n",
        "    else:\n",
        "        # Default behavior uses word-level tokenization.\n",
        "        # The vectorizer will automatically tokenize the text into words.\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        # For word-level analysis, join tokens with a space to form a proper sentence.\n",
        "        train_texts = [\" \".join(sent) for sent in splits[0]]\n",
        "        test_texts = [\" \".join(sent) for sent in splits[2]]\n",
        "\n",
        "    # Fit the vectorizer on the training data to learn the vocabulary and IDF values,\n",
        "    # then transform the training texts into TF-IDF feature matrices.\n",
        "    train_tfidf = vectorizer.fit_transform(train_texts)\n",
        "    # Use the learned vocabulary to transform the test texts into TF-IDF feature matrices.\n",
        "    test_tfidf = vectorizer.transform(test_texts)\n",
        "\n",
        "    return train_tfidf, test_tfidf\n"
      ],
      "metadata": {
        "id": "zq_ROpjoDQyQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a classifier we are going to employ Random forest (https://towardsdatascience.com/understanding-random-forest-58381e0602d2), a model that consists in an ensemble of Decision Trees that see different data by means of bagging and boosting."
      ],
      "metadata": {
        "id": "-sqESwVcDVwA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AjHmPMk74_o"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def get_accuracy(preds,labels):\n",
        "    return sum([p == l for p,l in zip(preds,labels)])/len(preds)*100\n",
        "\n",
        "def trainRandomForestClassifier(train_idx,test_idx):\n",
        "  train_idx = np.array(train_idx,dtype='float')\n",
        "  test_idx = np.array(test_idx,dtype='float')\n",
        "\n",
        "  clf = RandomForestClassifier(n_estimators=10, max_depth=3,random_state=SEED)\n",
        "  clf.fit(train_idx,train_targets)\n",
        "\n",
        "  preds = clf.predict(test_idx)\n",
        "  print('Accuracy', get_accuracy(preds,test_targets),'%')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBe3wqQ-74zB"
      },
      "source": [
        "**Train the model for the different preprocesses. How they affect the results?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhZLpbEA74n0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f49cf5bd-527e-44fd-91af-33b7160b14cb"
      },
      "source": [
        "# Tokenized at word level and preprocessed data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_tok_sents,valid_tok_sents,test_tok_sents],tok_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 70.456 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenized and character data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_char_sents,valid_char_sents,test_char_sents],char_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "05Pr156WDcwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPTLrOpxqHYi"
      },
      "source": [
        "#BPE tokenized data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_bpe_sents,valid_bpe_sents,test_bpe_sents],bpe_voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD8GEgGtqID7"
      },
      "source": [
        "# NOT tokenized data\n",
        "train_idx,valid_idx,test_idx = bag_of_words([train_sents,valid_sents,test_sents],voc)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the same experiment with TF-IDF and comparing the results**\n",
        "\n"
      ],
      "metadata": {
        "id": "5jW1wTKyDhtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, test_idx = compute_tfidf([train_tok_sents, valid_tok_sents, test_tok_sents])\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "WCMFGuyIDkd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, test_idx = compute_tfidf([train_char_sents,valid_char_sents,test_char_sents],True)\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "Ws_XAS1uDlq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, test_idx = compute_tfidf([train_bpe_sents,valid_bpe_sents,test_bpe_sents])\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "0EPL02kxDm-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, test_idx = compute_tfidf([train_sents,valid_sents,test_sents])\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "OPp3zzvHDoR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we are going to try the BERT tokenizer with TF-IDF**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vB_-FH7VDqfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx, test_idx = compute_tfidf([bert_tokenized_train, bert_tokenized_valid, bert_tokenized_test])\n",
        "trainRandomForestClassifier(train_idx,test_idx)"
      ],
      "metadata": {
        "id": "gLwxC-FADsJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlvrxdXk74cT"
      },
      "source": [
        "# Conclusions\n",
        "* Preprocessing plays an important role in the performance of NLP systems.\n",
        "* Even for the same model performance may vary a lot depending on how expressive are the features selected for the task.\n",
        "* All methods  presented (with the exception of no preprocess) are employed in state of the art work for several tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3fsubjE74P-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}