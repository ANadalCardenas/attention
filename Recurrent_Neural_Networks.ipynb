{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANadalCardenas/attention/blob/main/Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgvzqC1IBjH"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "## Lab credit\n",
        "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2019).\n",
        "\n",
        "Updated by [Gerard I. GÃ¡llego](https://www.linkedin.com/in/gerard-gallego/) (2021), [Javier Ferrando](https://www.linkedin.com/in/javierferrandomonsonis/) (2022), and [Ioannis Tsiamas](https://www.linkedin.com/in/i-tsiamas/) (2023).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqBBz46awSAh"
      },
      "source": [
        "# The Fault in Our Time\n",
        "\n",
        "This lab session introduces our beloved friends, the [recurrent neural networks (RNNs)](https://en.wikipedia.org/wiki/Recurrent_neural_network). Concretely, the topology we will be seeing is the Elman type, nowadays widely known plainly as RNN. Recurrent neural networks are the super cool queens of sequences: they know about order in sequences. As a quick test for how important sequential context is, and to prove that it is also very important for you... **CAN YOU TELL THE SIXTH DIGIT OF YOUR MOBILE PHONE NUMBER? WHAT PROCESS ARE YOU FOLLOWING TO RECALL IT?** Exactly, you went straight from the beginning of the full sequence, hence this is how important sequences are to us too :)\n",
        "\n",
        "As in the example before, we always work with sequences when using RNNs. In each batch of data, we have as many elements as the length of the sequence (seq_len), and each of these elements can contain multiple features (num_feats):\n",
        "\n",
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/input_batch.png?raw=true\" class=\"center\" title=\"input batch\" width=\"300\"/>\n",
        "</p><br>\n",
        "\n",
        "A fully connected layer is defined as:\n",
        "\n",
        "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{b})$\n",
        "\n",
        "With \"only one\" (but super important) change we formulate the RNN:\n",
        "\n",
        "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{U}\\boldsymbol{h}_{t-1} + \\boldsymbol{b})$\n",
        "\n",
        "Exactly, we added the matrix $\\boldsymbol{U}$ which is a set of connections among all the neurons from the hidden layers to themselves (hence a feedback)!\n",
        "\n",
        "This looks like the following, which is typically unrolled in time to show both flows of data, feed-forward ($\\boldsymbol{W}$) + time ($\\boldsymbol{U}$):\n",
        "\n",
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/one_layer_rnn.png?raw=true\" class=\"center\" title=\"one layer RNN\" width=\"300\"/>\n",
        "</p><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ju0_6RCtxOw_"
      },
      "outputs": [],
      "source": [
        "# Let's first import the typical stuff to play with deep nets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "torch.manual_seed(1)\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.cuda.manual_seed_all(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8YHdo2o0Ago"
      },
      "source": [
        "### Exercise 1: Building a recurrent neural layer\n",
        "\n",
        "In the next cell, we will define our own unidirectional RNN layer. The class `MyUnidirectionalRNN` must make use of `nn.Linear` layers to make the feed-forward and time projections, and use the `nn.Parameter` class to build the biases. Please build the recurrent neural component with the addition of the recurrent connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD-28w3YxOxB",
        "outputId": "5cbe1828-933d-4ad3-fd07-caa63322bdc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Output shape: 5 sequences, each of length 15, each token with 32 dims\n"
          ]
        }
      ],
      "source": [
        "class MyUnidirectionalRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, num_feats, rnn_size=128):\n",
        "    super().__init__()\n",
        "    self.rnn_size = rnn_size\n",
        "\n",
        "    # Definition of the RNN parameters with the use of Linear layers:\n",
        "\n",
        "    # Define the input activation matrix W\n",
        "    self.W = nn.Linear(num_feats, rnn_size, bias=False)\n",
        "\n",
        "    # TODO: Define the hidden activation matrix U\n",
        "    self.U = nn.Linear(rnn_size, rnn_size, bias=False)\n",
        "\n",
        "    # Define the bias\n",
        "    self.b = nn.Parameter(torch.zeros(1, rnn_size))\n",
        "\n",
        "  def forward(self, x, state=None):\n",
        "    # Assuming x is of shape [batch_size, seq_len, num_feats]\n",
        "    xs = torch.chunk(x, x.shape[1], dim=1)\n",
        "    hts = []\n",
        "    if state is None:\n",
        "      state = self.init_state(x.shape[0])\n",
        "    ht = state\n",
        "    for xt in xs:\n",
        "      # turn x[t] into shape [batch_size, num_feats] to be projected\n",
        "      xt = xt.squeeze(1)\n",
        "      # RNN formulation\n",
        "      ht = F.tanh(self.W(xt) + self.U(ht) + self.b)\n",
        "      # give the temporal dimension back to h[t] to be cated\n",
        "      hts.append(ht.unsqueeze(1))\n",
        "    hts = torch.cat(hts, dim=1)\n",
        "    return hts\n",
        "\n",
        "  def init_state(self, batch_size):\n",
        "    return torch.zeros(batch_size, self.rnn_size)\n",
        "\n",
        "# To correctly assess the answer, we build an example RNN with 10 inputs and 32 neurons\n",
        "rnn = MyUnidirectionalRNN(10, 32)\n",
        "# Then we will forward some random sequences, each of length 15\n",
        "xt = torch.randn(5, 15, 10)\n",
        "# The returned tensor will be h[t]\n",
        "ht = rnn(xt)\n",
        "assert ht.shape[0] == 5 and ht.shape[1] == 15 and ht.shape[2] == 32, \\\n",
        "'Something went wrong within the RNN :('\n",
        "print('Success! Output shape: {} sequences, each of length {}, each '\\\n",
        "      'token with {} dims'.format(ht.shape[0], ht.shape[1], ht.shape[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnEZ6UmPAJU"
      },
      "source": [
        "### But Why Would You Do That?\n",
        "\n",
        "Congratz on finishing your first RNN definition! Now you should understand a bit more on the intrinsics of our sequential friends. But why would you define your own RNN? We didn't even operate with a GPU. We didn't even consider that possibility. So in the real world, we use PyTorch's `nn.RNN`, which allows for building a **stack of RNN layers directly**. Let's see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmzclzrGxOxC",
        "outputId": "b6fb7293-6120-41bf-8821-b8f35ed2f08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(10, 128)\n",
            "Output h[t] tensor shape:  torch.Size([25, 5, 128])\n",
            "Output state tensor shape:  torch.Size([1, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# we will work with 10 input features\n",
        "NUM_FEATS = 10\n",
        "# and sequences of length 25\n",
        "SEQ_LEN = 25\n",
        "# and 5 samples per batch\n",
        "BATCH_SIZE = 5\n",
        "# and 128 neurons\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# The first RNN contains a single layer\n",
        "rnn1 = nn.RNN(NUM_FEATS, HIDDEN_SIZE)\n",
        "print(rnn1)\n",
        "\n",
        "# Now let's build a random input tensor to forward through it\n",
        "xt = torch.randn(SEQ_LEN, BATCH_SIZE, NUM_FEATS)\n",
        "ht, state = rnn1(xt)\n",
        "print('Output h[t] tensor shape: ', ht.shape)\n",
        "print('Output state tensor shape: ', state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg4c-TaLQ5Ag"
      },
      "source": [
        "#### OK STOP IT HERE, We've got to talk\n",
        "\n",
        "Think about how many things are happening in the previous cell. First, we define some hyper-parameters to define the input tensor shape and the RNN size. Then, we build one RNN layer. Then, we build random data. Finally, we forward the random data, and what is returned? Why does the input tensor `x` have that shape? Why is the RNN returning 2 output values?\n",
        "\n",
        "**First answer:** The input data to an RNN can be shaped in 2 formats: `batch_first=True` and `batch_first=False`. As its name indicates, when it is `False`, the `batch_size` dimension is not the first but the second one. Then which is the first one? The `sequence_length`. If we do not specify anything, by default `batch_first=False`, so the tensor $\\boldsymbol{x}_t$ must have the dimensions: [`seq_len`, `batch_size`, `num_feats`]. We normally use `batch_first=True` to couple the RNN easily with other layers like the `nn.Linear` one.\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Find the second answer on \"**Why is the RNN returning 2 output values?**\". Understand what is the `state` output and answer: \"**what does it contain?**\". Your source of knowledge is in the following URL, where the outputs description for the `RNN` module is given: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wAY1wWTT3pR"
      },
      "source": [
        "Now we can continue defining some more examples of RNN layers as promised before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdEXy9AWxOxC",
        "outputId": "bb79e3fc-a055-4d4d-abae-de1c87ad981f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN 2 layers >> ht.shape:  torch.Size([25, 5, 128])\n",
            "RNN 2 layers >> state.shape:  torch.Size([2, 5, 128])\n",
            "RNN 2 layers, batch_first >> ht.shape:  torch.Size([5, 25, 128])\n",
            "RNN 2 layers, batch_first >> state.shape:  torch.Size([2, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# 2 Layer RNN\n",
        "rnn2 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2)\n",
        "ht, state = rnn2(xt)\n",
        "print('RNN 2 layers >> ht.shape: ', ht.shape)\n",
        "print('RNN 2 layers >> state.shape: ', state.shape)\n",
        "\n",
        "# Batch Size first RNN\n",
        "xt_bf = torch.randn(BATCH_SIZE, SEQ_LEN, NUM_FEATS)\n",
        "rnn3 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2, batch_first=True)\n",
        "ht, state = rnn3(xt_bf)\n",
        "print('RNN 2 layers, batch_first >> ht.shape: ', ht.shape)\n",
        "print('RNN 2 layers, batch_first >> state.shape: ', state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNGrWTyorkNg"
      },
      "source": [
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/two_layers_rnn.png?raw=true\" class=\"center\" title=\"two layers RNN\" width=\"300\"/>\n",
        "</p><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfRkpNiOUwxA"
      },
      "source": [
        "### Exercise 3.1\n",
        "Build a **bidirectional RNN with 3 layers** by completing the TODO in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOMzDtxSxOxD",
        "outputId": "ee80c3ea-faba-4ae1-b67f-43115c2c62cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional RNN layer >> bi_ht.shape:  torch.Size([5, 25, 256])\n",
            "Bidirectional RNN layer >> bi_state.shape:  torch.Size([6, 5, 128])\n"
          ]
        }
      ],
      "source": [
        "# TODO: build the bidirectional RNN layer\n",
        "bi_rnn = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=3, batch_first=True, bidirectional=True)\n",
        "\n",
        "# forward xt_bf\n",
        "bi_ht, bi_state = bi_rnn(xt_bf)\n",
        "print('Bidirectional RNN layer >> bi_ht.shape: ', bi_ht.shape)\n",
        "print('Bidirectional RNN layer >> bi_state.shape: ', bi_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDE81Z2XKN2J"
      },
      "source": [
        "### Exercise 3.2\n",
        "What is the output $\\boldsymbol{h}_t$ shape and why?\n",
        "\n",
        "### Exercise 3.3\n",
        "What is the output `state` shape and why?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgKP_9eKTQcp"
      },
      "source": [
        "### Hold The Gates! A Recurrent Re-Evolution\n",
        "\n",
        "You've surely heard about the `LSTM` or the `GRU`, two practically sibling recurrent models. Well those are the actual RNNs you will use in your everyday. Why? Because they:\n",
        "1. Improve the memory capacity of the RNN.\n",
        "2. Improve the gradient flow of vanilla RNNs thanks to the learnable gate mechanisms.\n",
        "\n",
        "An LSTM or GRU cell is a composition of different neurons working jointly, and the whole thing replaces a single RNN neuron. The RNN cell (with one $\\tanh$ neuron), the LSTM cell and the GRU cell are depicted in the following figure from [this article](https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwiMiPbfoPHlAhUQCxoKHW9qA04Qjhx6BAgBEAI&url=http%3A%2F%2Fdprogrammer.org%2Frnn-lstm-gru&psig=AOvVaw3mU76KRvFfY9WiOF4N12ex&ust=1574080203478260):\n",
        "\n",
        "<p align=\"center\"><br>\n",
        "<img src=\"https://github.com/telecombcn-dl/labs-all/blob/main/labs/rnn/images/RNN-vs-LSTM-vs-GRU.png?raw=true\" class=\"center\" title=\"two layers RNN\" width=\"1000\"/>\n",
        "</p><br>\n",
        "\n",
        "Now check that out. In the case of the LSTM, we have **two signals flowing in time** apart from the feed-forward input per time-step: $\\boldsymbol{c}_t$ and $\\boldsymbol{h}_t$. The first one is called the cumulative cell state. It basically will add everything it is \"allowed to see\" from the input, and will forget portions of it. This is unbounded. On the other hand, the output cell state $\\boldsymbol{h}_t$ will be the final layer activation (what is allowed to come out of it). This is bounded [-1, 1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WhMLsykdUEq"
      },
      "source": [
        "### Exercise 4: An LSTM Character-based Language Model\n",
        "\n",
        "In this final exercise we will train a language model that will work at the character level. This is, a neural network based on an RNN architecture that will complete language (textual) sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFGtDsWkUZoK"
      },
      "source": [
        "Our dataset will be composed of scripts from the *Friends* TV show. Download the episode 1 trainset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YFQYWd1D5mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zIoys8XBxOxE"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/telecombcn-dl/labs-all/master/labs/rnn/episode1_english.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfSGpB8ExOxE",
        "outputId": "5dad383b-23a7-438c-e863-c08b20536812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of found vocabulary tokens:  67\n"
          ]
        }
      ],
      "source": [
        "# Let's prepare some synthetic data\n",
        "\n",
        "def prepare_sequence(seq, char2idx, onehot=True):\n",
        "    # convert sequence of words to indices\n",
        "    idxs = [char2idx[c] for c in seq]\n",
        "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
        "    if onehot:\n",
        "      # conver to onehot (if input to network)\n",
        "      ohs = F.one_hot(idxs, len(char2idx)).float()\n",
        "      return ohs\n",
        "    else:\n",
        "      return idxs\n",
        "\n",
        "with open('episode1_english.txt', 'r') as txt_f:\n",
        "  training_data = [l.rstrip() for l in txt_f if l.rstrip() != '']\n",
        "\n",
        "# merge the training data into one big text line\n",
        "training_data = '$'.join(training_data)\n",
        "\n",
        "# Assign a unique ID to each different character found in the training set\n",
        "char2idx = {}\n",
        "for c in training_data:\n",
        "    if c not in char2idx:\n",
        "        char2idx[c] = len(char2idx)\n",
        "idx2char = dict((v, k) for k, v in char2idx.items())\n",
        "VOCAB_SIZE = len(char2idx)\n",
        "RNN_SIZE = 1024\n",
        "MLP_SIZE = 2048\n",
        "SEQ_LEN = 50\n",
        "print('Number of found vocabulary tokens: ', VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehk_6JNIkZOs"
      },
      "source": [
        "##### Exercise 4.1\n",
        "* What is the amount of outputs needed by the character prediction model?\n",
        "\n",
        "##### Exercise 4.2\n",
        "* What is the proper activation to plug on top of the MLP (if any)? (Note that we use [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) later on).\n",
        "\n",
        "##### Exercise 4.3\n",
        "* Finish the definition of the `CharLSTM` model to include a `nn.LSTM` layer, with `batch_first=True`, `vocab_size` inputs and `rnn_size` cells, and an MLP that projects the `rnn_size` to `mlp_size` with one `ReLU` hidden layer and then to the appropriate amount of outputs. Put a `Dropout(0.4)` after the `ReLU`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SsKNewULxOxE"
      },
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, rnn_size, mlp_size):\n",
        "        super().__init__()\n",
        "        self.rnn_size = rnn_size\n",
        "\n",
        "        # TODO: Define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=vocab_size,\n",
        "            hidden_size=rnn_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dout = nn.Dropout(0.4)\n",
        "\n",
        "        # TODO: Create an MLP with a hidden layer of mlp_size neurons that maps\n",
        "        # from the RNN hidden state space to the output space of vocab_size\n",
        "        self.mlp = nn.Sequential(\n",
        "          # Linear layer\n",
        "          nn.Linear(rnn_size, mlp_size),\n",
        "          # Activation function\n",
        "          nn.ReLU(),\n",
        "          # Dropout (0.4)\n",
        "          nn.Dropout(0.4),\n",
        "          # Linear layer\n",
        "          nn.Linear(mlp_size, vocab_size),\n",
        "          # Activation function\n",
        "          nn.LogSoftmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, sentence, state=None):\n",
        "        bsz, slen, vocab = sentence.shape\n",
        "        ht, state = self.lstm(sentence, state)\n",
        "        ht = self.dout(ht)\n",
        "        h = ht.contiguous().view(-1, self.rnn_size)\n",
        "        logprob = self.mlp(h)\n",
        "        return logprob, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K3L5BIyLrja"
      },
      "source": [
        "Test how the model performs when using randomly initialized weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ZV-iAoxOxF",
        "outputId": "b12e27ac-e662-4da5-855a-f8453e37c12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monica was FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n"
          ]
        }
      ],
      "source": [
        "# Let's build an example model and see what the scores are before training\n",
        "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
        "\n",
        "# This should output crap as it is not trained, so a fixed random tag for everything\n",
        "\n",
        "def gen_text(model, seed, char2idx, num_chars=150):\n",
        "  model.eval()\n",
        "  # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "      inputs = prepare_sequence(seed, char2idx)\n",
        "      # fill the RNN memory with the seed sentence\n",
        "      seed_pred, state = model(inputs.unsqueeze(0))\n",
        "      # now begin looping with feedback char by char from the last prediction\n",
        "      preds = seed\n",
        "      curr_pred = torch.argmax(seed_pred[-1, :])\n",
        "      curr_pred = idx2char[curr_pred.item()]\n",
        "      preds += curr_pred\n",
        "      for _ in range(num_chars):\n",
        "\n",
        "        # TODO: Get the next char prediction from the model given the current prediction and current state\n",
        "        char_idx = char2idx[curr_pred]\n",
        "        inputs = torch.zeros(1, 1, len(char2idx))\n",
        "        inputs[0, 0, char_idx] = 1.0\n",
        "        curr_pred, state = model(inputs, state)\n",
        "\n",
        "        curr_pred = torch.argmax(curr_pred[-1, :])\n",
        "        curr_pred = idx2char[curr_pred.item()]\n",
        "        if curr_pred == '$':\n",
        "          # special token to add newline char\n",
        "          preds += '\\n'\n",
        "        else:\n",
        "          preds += curr_pred\n",
        "      return preds\n",
        "\n",
        "\n",
        "print(gen_text(model, 'Monica was ', char2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_HuhKqRL0p-"
      },
      "source": [
        "Prepare the training data by defining the data batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMkuzcsSxOxF",
        "outputId": "61d23e39-7bf3-4bfa-a17b-94335c1e75e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training string len:  23149\n",
            "Sub-sequences len:  361\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "T = len(training_data)\n",
        "CHUNK_SIZE = T // BATCH_SIZE\n",
        "# let's first chunk the huge train sequence into BATCH_SIZE sub-sequences\n",
        "trainset = [training_data[beg_i:end_i] \\\n",
        "            for beg_i, end_i in zip(range(0, T - CHUNK_SIZE, CHUNK_SIZE),\n",
        "                                    range(CHUNK_SIZE, T, CHUNK_SIZE))]\n",
        "print('Original training string len: ', T)\n",
        "print('Sub-sequences len: ', CHUNK_SIZE)\n",
        "\n",
        "# The way training works is the following:\n",
        "# at each batch sampling from the trainset, we pick a portion of sequences\n",
        "# continuous with a sliding window in time. Hence, each of the BATCH_SIZE sub-sequences\n",
        "# in batch b[i] will continue in batch b[i + 1] in the same position of the batch dimension.\n",
        "# This is called stateful sampling, where we train with consecutive windows of sequences\n",
        "# We broke the long string into BATCH_SIZE subsequence, so we introduced BATCH_SIZE - 1\n",
        "# discontinuities... YES. But we can assume that each sub-sequence is continuous in a long\n",
        "# enough chunk so that those discontinuities are negligible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VVeGkjk5bs"
      },
      "source": [
        "##### Exercise 4.4\n",
        "\n",
        "What is the length of the sliding window that will run over each of the training sub-sequences?\n",
        "\n",
        "NOTE: it is defined as a hyper-parameter above. How is this related to the backpropagation through time (BPTT)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZsDy8QbYxOxG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a68aaa47-8cec-447c-c3ae-5e3712ef0b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "They wand to the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with\n",
            "------------------------------\n",
            "Finished epoch 50 in 0.3 s: loss: 1.587183\n",
            "------------------------------\n",
            "They whink you re for her whink your on ther and then she ne toding to this guy wain.\n",
            "Monica: Oh wait, wait, unless you hase and the chand the door and a pr\n",
            "------------------------------\n",
            "Finished epoch 100 in 0.3 s: loss: 0.295691\n",
            "------------------------------\n",
            "They whink you rub alr how you doey.\n",
            "Ross: Ooh... What, I just tho gotta go, I got a date with Andrea--Angela--Andrea--Angela--Andrea--Angela--Andrea--Angel\n",
            "------------------------------\n",
            "Finished epoch 150 in 0.3 s: loss: 0.047934\n",
            "------------------------------\n",
            "They whink it wholkn what're you in to thing.\n",
            "Joey: (cometting up and bots to be sore.)\n",
            "Chandler: Plook, Ross, look. You're founing.]\n",
            "Ross: C'mon, ut, caule\n",
            "------------------------------\n",
            "Finished epoch 200 in 0.3 s: loss: 0.026335\n",
            "------------------------------\n",
            "They whink you re moze that is a let mor shiel I'm just gonna get up and go to work.\n",
            "Rachel: Oh, look, wish, be I stung the coffee, prink in, or cuts com, I\n",
            "------------------------------\n",
            "Finished epoch 250 in 0.3 s: loss: 0.016443\n",
            "------------------------------\n",
            "They weren't know and doing this going to tho denvering, thing s. Paul whe bind Guys wayt mean with metsens. (Ros and Ros as of the bouck to the door so the\n",
            "------------------------------\n",
            "Finished epoch 300 in 0.3 s: loss: 0.016274\n",
            "------------------------------\n",
            "They here that it in probus, this is Paul.\n",
            "All: Ohay. (They kits) Thank you. (Exits.)\n",
            "Ross: Y'know, I figure if I can make coffee, there isn't anything I ca\n",
            "------------------------------\n",
            "Finished epoch 350 in 0.3 s: loss: 0.013861\n",
            "------------------------------\n",
            "They was exly books and dee it tolking ar that's hig in Joey and Joey:  Push her down the stairs! Push her down the stairs!\n",
            "(She stapsenterting a pitnle and\n",
            "------------------------------\n",
            "Finished epoch 400 in 0.3 s: loss: 0.014114\n",
            "------------------------------\n",
            "They what what wo dn't do that do you bo the gonds that was a long!\n",
            "Monica: Joey, stop hitting on her! It's her wedding day!\n",
            "Joey: What, like there we wed t\n",
            "------------------------------\n",
            "Finished epoch 450 in 0.3 s: loss: 0.014266\n",
            "------------------------------\n",
            "They was ng live her some coffee were something wrong with him!\n",
            "Chandler: Al right Joey, be nice.  So does he have  hump? A hump and a hairpiece?\n",
            "Phoebe: Wa\n",
            "------------------------------\n",
            "Finished epoch 500 in 0.3 s: loss: 0.010453\n",
            "------------------------------\n",
            "They was no lak with sithing.\n",
            "Joey: (comorting her) And hey, you need anything, you can alays lome to Joey. Me and Chandler live across the beonch fith the \n",
            "------------------------------\n",
            "Finished epoch 550 in 0.3 s: loss: 0.009879\n",
            "------------------------------\n",
            "They here the hid that I was more turned on by this! I'm sorry. I just don't love him. Well, it miget, cumere candsent. (She starts to pluck at the air just\n",
            "------------------------------\n",
            "Finished epoch 600 in 0.3 s: loss: 0.009078\n",
            "------------------------------\n",
            "They wey noed out of my freezer! Whe here worka to be tha got that o thing the candles with a hat here welling a little ahead of selvize.\n",
            "Where she whone So\n",
            "------------------------------\n",
            "Finished epoch 650 in 0.3 s: loss: 0.007074\n",
            "------------------------------\n",
            "They was ng snip in a lot?\n",
            "Monica: Joey, stop hitting on her! It's her wedding day!\n",
            "Joey: What, like this guy wewcon out of the can, I should have known.\n",
            "Ph\n",
            "------------------------------\n",
            "Finished epoch 700 in 0.3 s: loss: 0.007605\n",
            "------------------------------\n",
            "They wast way 't ingit. (to be coffer Gow. No, I'm not you don't love it you meanta be ling out the here and you never her a beed was no loom that morning w\n",
            "------------------------------\n",
            "Finished epoch 750 in 0.3 s: loss: 0.009164\n",
            "------------------------------\n",
            "They was no sa- (Horgs to hangel to going on't me, in's up it my aue, about, and that's when it hit me: how much Barry loks like Mr. Potato Head. Y'know, I \n",
            "------------------------------\n",
            "Finished epoch 800 in 0.3 s: loss: 0.007458\n",
            "------------------------------\n",
            "They wey not do you doing today? Did you mean hine it what he is working o figure out what is going on.]\n",
            "Monica: Now I'm guesing that he bought her the big \n",
            "------------------------------\n",
            "Finished epoch 850 in 0.3 s: loss: 0.005631\n",
            "------------------------------\n",
            "They wey noed that wrone worng of her! (She starts mossaging them.)\n",
            "Monica: I just thought I was Monica's geeky olde brother.\n",
            "Rachel: I did.\n",
            "Ross: Oh. Liste\n",
            "------------------------------\n",
            "Finished epoch 900 in 0.3 s: loss: 0.006929\n",
            "------------------------------\n",
            "They werd to be that was a lobe.\n",
            "Monica: Joey, stop hitting on that? She didn't know, how should I know?\n",
            "Chandler: Paul, wish me luck!\n",
            "Monica: What for?\n",
            "Rac\n",
            "------------------------------\n",
            "Finished epoch 950 in 0.3 s: loss: 0.008701\n",
            "------------------------------\n",
            "They were welking a lottle a beat of your found John and Davide this is and Paul the pine and toey here tonight. Joey and Chandler are coming over to her me\n",
            "------------------------------\n",
            "Finished epoch 1000 in 0.3 s: loss: 0.005077\n",
            "------------------------------\n",
            "They well with this guy with a big hammer said you might be here watch!\n",
            "Paul: So you want toen go bo I want a bouy down think that was my point!\n",
            "Ross: You k\n",
            "------------------------------\n",
            "Finished epoch 1050 in 0.3 s: loss: 0.008732\n",
            "------------------------------\n",
            "They well me look.\n",
            "(The sarp thes like to ther here today.\n",
            "Joey: Ohh.\n",
            "Monica: (to Ros) Let me get you some coffee.\n",
            "Ross: Thanks.\n",
            "Phoebe: Oh, I wish I could,\n",
            "------------------------------\n",
            "Finished epoch 1100 in 0.3 s: loss: 0.005552\n",
            "------------------------------\n",
            "They weren't looking at you before?!\n",
            "Chandler: So walk you something whong with her!\n",
            "[Time Lapse]\n",
            "Chandler: Alright, kids, I gotta get to work. If I don't i\n",
            "------------------------------\n",
            "Finished epoch 1150 in 0.3 s: loss: 0.004882\n",
            "------------------------------\n",
            "They was no have goica) That'd bother going to get and there al share...\n",
            "Chandler: That's right.\n",
            "Joey: Rachel!  That was a library card!\n",
            "All: Cut, cut, cut,\n",
            "------------------------------\n",
            "Finished epoch 1200 in 0.3 s: loss: 0.004747\n",
            "------------------------------\n",
            "They was no start?  Io so what I just thought I was Monica's geeky olden to this is Paul's watch and goes it to leart.\n",
            "Joey: Look it was Paul starting and r\n",
            "------------------------------\n",
            "Finished epoch 1250 in 0.3 s: loss: 0.005652\n",
            "------------------------------\n",
            "They was no lake thing trean her dive on a real wath?\n",
            "Monica: Shut up, and put my able to just leave of a woman for four years.   Four yers of closeness and\n",
            "------------------------------\n",
            "Finished epoch 1300 in 0.3 s: loss: 0.004216\n",
            "------------------------------\n",
            "They was no stap in his urt of my lair.\n",
            "(Ross gestures are exteoning trothe bard with out, cherester and Chandler live across the phone.) Hi, machine cut me\n",
            "------------------------------\n",
            "Finished epoch 1350 in 0.3 s: loss: 0.003855\n",
            "------------------------------\n",
            "They werd the look. You're feeling a lot of pan right now. You're angry. You're hurting. Can I think I cut do you think it wit me going it wat a job all rig\n",
            "------------------------------\n",
            "Finished epoch 1400 in 0.3 s: loss: 0.007749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "They wercome it hat me now what to be ine!\n",
            "Monica: (to Ros) Let me get you some coffee.\n",
            "Ross: Thanks.\n",
            "Phoebe: Ooh, I just pulled out four eyelahs.)\n",
            "[Scene: \n",
            "------------------------------\n",
            "Finished epoch 1450 in 0.3 s: loss: 0.006817\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They was no like a head out of the window.)\n",
            "[Cut to Rachel starts to pluck at the air just in hat washow you ou Lean I love with your socks on, but it isn't\n",
            "------------------------------\n",
            "Finished epoch 1500 in 0.3 s: loss: 0.003471\n",
            "------------------------------\n",
            "They we cendle be kitcher cream of you.\n",
            "Monica: That's Paul's watch and goes thing out of a suw. Y'know, I mean, I always to Chandler are working as thaing \n",
            "------------------------------\n",
            "Finished epoch 1550 in 0.3 s: loss: 0.005410\n",
            "------------------------------\n",
            "They was not that have abealy not that have auy! And met that's Paul.\n",
            "Monica: Oh God, is it 6:3?\n",
            "Rachel: Hey Mon, look what I just found on thelcome to the \n",
            "------------------------------\n",
            "Finished epoch 1600 in 0.3 s: loss: 0.004481\n",
            "------------------------------\n",
            "They was now weat me and the this guy with a big hammer said you might be here watch!\n",
            "Paul: So you wand Joey, (to Rachel) This is everybody, this is Rachel,\n",
            "------------------------------\n",
            "Finished epoch 1650 in 0.3 s: loss: 0.004009\n",
            "------------------------------\n",
            "They were weening your honeymoon, God.. No, no, although, Arubame spot over and over and over again until it stats and the kitchen and says to Chandler are \n",
            "------------------------------\n",
            "Finished epoch 1700 in 0.3 s: loss: 0.003916\n",
            "------------------------------\n",
            "They when sou that is an 'L'-shape bracket.\n",
            "Joey: Which goes where?\n",
            "Chandler: I have some sort of beacon thing then hat hat her he sucked or something... (J\n",
            "------------------------------\n",
            "Finished epoch 1750 in 0.3 s: loss: 0.005096\n",
            "------------------------------\n",
            "They was only one woman- for her...\n",
            "Joey: What for?\n",
            "Rachel: I'm gonna go get one of those (Thinks) job things.\n",
            "(Monica stomps on Paul's watch and goes at sh\n",
            "------------------------------\n",
            "Finished epoch 1800 in 0.3 s: loss: 0.003478\n",
            "------------------------------\n",
            "They cleaning fin. I knoway...\n",
            "Monica: Are you okay, seepped and I at have and of my here, (me is it?\n",
            "Rachel: (oo phone) Daddy, I just... I can't marriel's,\n",
            "------------------------------\n",
            "Finished epoch 1850 in 0.3 s: loss: 0.003326\n",
            "------------------------------\n",
            "They was Rachel is there.]\n",
            "Joey: (sitting on the arm of a doure and my start, but I think I'm just gonna how buck you just stip taybull me!\n",
            "[Time Lapse, Ros\n",
            "------------------------------\n",
            "Finished epoch 1900 in 0.3 s: loss: 0.005197\n",
            "------------------------------\n",
            "They were watch!\n",
            "Monica: Show!\n",
            "Paul: I know, I know, I'm such n tiou. I just stopped and I said, 'What if I wanna be a- a proe, and I got her, and I didn't \n",
            "------------------------------\n",
            "Finished epoch 1950 in 0.3 s: loss: 0.004128\n",
            "------------------------------\n",
            "They chenge to the wedding.\n",
            "Rachel: Oh God... well, but st in the wark.\n",
            "Joey: Look, I know that some girl is with me and Paul are still eat Bing! Cherry Van\n",
            "------------------------------\n",
            "Finished epoch 2000 in 0.3 s: loss: 0.002511\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'NLLLoss')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's now build a model to train with its optimizer and loss\n",
        "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
        "model.to(device)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "NUM_EPOCHS = 2000\n",
        "tr_loss = []\n",
        "state = None\n",
        "timer_beg = timer()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  # let's slide over our dataset\n",
        "  for beg_t, end_t in zip(range(0, CHUNK_SIZE - SEQ_LEN - 1, SEQ_LEN + 1),\n",
        "                          range(SEQ_LEN + 1, CHUNK_SIZE, SEQ_LEN + 1)):\n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "    # Tensors of one-hot sequences.\n",
        "    for sent in trainset:\n",
        "      # chunk the sentence\n",
        "      chunk = sent[beg_t:end_t]\n",
        "\n",
        "      # TODO: get X and Y with a shift of 1\n",
        "      X = chunk[:-1]    # remove last char\n",
        "      Y = chunk[1:]    # remove first char\n",
        "\n",
        "      # convert each sequence to one-hots and labels respectively\n",
        "      X = prepare_sequence(X, char2idx)\n",
        "      Y = prepare_sequence(Y, char2idx, onehot=False)\n",
        "      dataX.append(X.unsqueeze(0)) # create batch-dim\n",
        "      dataY.append(Y.unsqueeze(0)) # create batch-dim\n",
        "    dataX = torch.cat(dataX, dim=0).to(device)\n",
        "    dataY = torch.cat(dataY, dim=0).to(device)\n",
        "\n",
        "    # Step 3. Run our forward pass.\n",
        "    # Forward through model and carry the previous state forward in time (statefulness)\n",
        "    y_, state = model(dataX, state)\n",
        "    # detach the previous state graph to not backprop gradients further than the BPTT span\n",
        "    state = (state[0].detach(), # detach c[t]\n",
        "             state[1].detach()) # detach h[t]\n",
        "\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    #  calling optimizer.step()\n",
        "    loss = loss_function(y_, dataY.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss.append(loss.item())\n",
        "  timer_end = timer()\n",
        "  if (epoch + 1) % 50 == 0:\n",
        "    # Generate a seed sentence to play around\n",
        "    model.to('cpu')\n",
        "    print('-' * 30)\n",
        "    print(gen_text(model, 'They ', char2idx))\n",
        "    print('-' * 30)\n",
        "    model.to(device)\n",
        "    print('Finished epoch {} in {:.1f} s: loss: {:.6f}'.format(epoch + 1,\n",
        "                                                               timer_end - timer_beg,\n",
        "                                                               np.mean(tr_loss[-10:])))\n",
        "  timer_beg = timer()\n",
        "\n",
        "plt.plot(tr_loss)\n",
        "plt.xlabel('Update')\n",
        "plt.ylabel('NLLLoss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iulcm9gPNhwK"
      },
      "source": [
        "Now that the generator of characters is trained, we can ask it to predict the rest of a sentence that begins with `Professor `:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iKc6EzH1xOxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f06e23-1798-4c9a-d230-94ccef35d86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Professor sore maring when he istarting at me!\n",
            "It's like it's like off.) (Chandler hits what he is working o fingrout woman for everybody, y'know? I mean what if you get through it?\n",
            "Paul: Well, you might be here watch!\n",
            "Paul: So you know how long it's been since I've grabbed a spoon? Do the words 'Billy, don't be a hero' mean, I had no wedathere my new furniture.]\n",
            "Ross: I'm divorced!  I'm such I was Finging, I was Monica's geeky olde boothers!\n",
            "Chandler: Oo, this is a Dear Diary moment.\n",
            "Monica: Rach, wait, unless you a peane wothe wotch.\n",
            "Monica: You and I have kinda drifted apart, but out, and that's when it hit me: how much Barry loks like Mr. Potato Head. Y'know, I mean, I always to just and that is why we don't do that grabing the best pare with this guys with a big hammer said you might be here watch!\n",
            "Paul: So you know Poul like you gett her the big pipe organ, and shere's hat me and Chandler are coming over to her my me puy the anster in the park.\n",
            "Joey: Look it was a job all right?\n",
            "Chandler: '\n"
          ]
        }
      ],
      "source": [
        "print(gen_text(model.to('cpu'), 'Professor ', char2idx, 1000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Think of the RNN like this (2 layers, 3 time steps):\n",
        "\n",
        "Layer 2:   h2(1) â h2(2) â h2(3)\n",
        "             â       â       â\n",
        "Layer 1:   h1(1) â h1(2) â h1(3)\n",
        "             â       â       â\n",
        "Input:      x(1)     x(2)     x(3)\n",
        "\n",
        "What PyTorch returns:\n",
        "ð¹ h_t (output)\n",
        "[h2(1), h2(2), h2(3)]\n",
        "\n",
        "\n",
        "â all time steps, last layer only\n",
        "\n",
        "ð¹ state\n",
        "[\n",
        "  h1(3),   # layer 1, last time step\n",
        "  h2(3)    # layer 2, last time step\n",
        "]"
      ],
      "metadata": {
        "id": "QvKWnaQjKiLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVRsFgHZa3f3"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3.7.10 ('trading')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "148c72a8fa8931f1b4adec61e5c626da15d84fdba20a1a50eaf0317d3b0337d5"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}